{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae2e905",
   "metadata": {},
   "source": [
    "## DATATHON CHALLENGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f38002",
   "metadata": {},
   "source": [
    "### _Predicting the Energ_Kcal using regression model_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3156ad1",
   "metadata": {},
   "source": [
    "The model predicts the Energ_Kcal based on the top 30 numerical predictors in the dataframe. Multiple categorical columns were dropped as the information to calculate energy was already provided in the form of the ingredients. The columns with more than 40% missing values were removed from the dataset and the missing values were imputed using KNN imputer for a better result. The outliers were not removed from the dataset due to the lack of domain knowledge. The features are mostly extremely right skewed. As the dataset had multiple variables with differing scale, data was normalized using the MinMaxScaler(). Multiple regression models were fit on the training data to evaluate the model performance. Even though the linear regression model gave the least MAE, the XGB regession model is selected as the final model as it is robust to outliers which we haven't removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data= pd.read_excel(r\"C:\\Users\\saras\\Downloads\\ABBREV.xlsx\\ABBREV.xlsx\")\n",
    "print(\"Data size: \",data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda659f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the datatype and missing value details\n",
    "print(data.info())\n",
    "data.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c98d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy=data\n",
    "\n",
    "#Dropping columns with more than 40% missing values\n",
    "data_copy= data_copy.drop(data_copy.columns[data_copy.isnull().mean()>0.4],axis=1)\n",
    "data_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20a660",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Dropping the short description as we are interested in calculating the calories for which all the ingredient details are \n",
    "#given. The same is being done for GmWt_Desc1 since we have the GmWt which gives the weight of the product\n",
    "#'NDB_No is dropped as it is similar to index and doesnt hold any meaning\n",
    "data_copy=data_copy.drop(['Shrt_Desc','GmWt_Desc1','NDB_No'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing missing values using KNN imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "X=data_copy.values\n",
    "imputer=KNNImputer()\n",
    "imputer.fit(X)\n",
    "#Out y do not have any missing values and hence not fitting and transforming y\n",
    "X_imputed=imputer.transform(X)\n",
    "X_imputed.shape\n",
    "data_copy= pd.DataFrame(X_imputed, columns= data_copy.columns)\n",
    "data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are building a model without removing the outliers\n",
    "#Checking skewness and plotting histogram \n",
    "skew=data_copy.skew()\n",
    "skew=skew.loc[lambda x:(x>5) | (x<-5)]\n",
    "print(\"Skewness before transformation: \",skew)\n",
    "\n",
    "data_copy.hist(figsize=(18,20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d648c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the data is highly skewed and on different scales, we will normalize the data using Minmaxscaler() \n",
    "#after splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X=data_copy.drop(['Energ_Kcal'],axis=1).values\n",
    "y=data_copy['Energ_Kcal'].values\n",
    "X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.333,random_state=42)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd70981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming independent variables using MinMaxScaler() as our data has variables having different scale\n",
    "scaler= MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming target variables using MinMaxScaler()\n",
    "scaler= MinMaxScaler()\n",
    "scaler.fit(y_train.reshape(-1,1))\n",
    "y_train=scaler.transform(y_train.reshape(-1,1))\n",
    "y_test= scaler.transform(y_test.reshape(-1,1))\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7efe977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the top 30 features for modelling with SelectKBest\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "fs = SelectKBest(score_func=f_regression, k=30)\n",
    "fs.fit(X_train, y_train.ravel())\n",
    "X_train_fs = fs.transform(X_train)\n",
    "X_test_fs = fs.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelling with multiple algorithms to select the best\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "models= [('LR', LinearRegression()), ('DTR',DecisionTreeRegressor()),('SVM',SVR()),('XG',XGBRegressor())]\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for name,model in models:\n",
    "    cv_results= cross_val_score(model,X_train_fs,y_train.ravel(),cv=kfold,scoring='neg_mean_absolute_error')\n",
    "    cv_results= np.absolute(cv_results)\n",
    "    print(\"{}: Mean error:{},\\t\\tStd deviation of error:{}\".format(name, cv_results.mean().round(4), cv_results.std().round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b456a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing XGB as the final model as it has a low MAE and are also robust to outliers\n",
    "\n",
    "model= XGBRegressor()\n",
    "model.fit(X_train_fs,y_train.ravel())\n",
    "ypred= model.predict(X_test_fs)\n",
    "print(mean_squared_error(y_test,ypred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
